import json
import re

with open('data/my_posts.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

posts = data.get('posts', [])

output_lines = []
output_lines.append(f"–í—Å–µ–≥–æ –ø–æ—Å—Ç–æ–≤: {len(posts)}")
output_lines.append("")

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è "—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫" (–¥–µ–π—Å—Ç–≤–∏—è-–ø—Ä–∏–∑—ã–≤—ã)
link_text_patterns = [
    r'[‚û°Ô∏è‚Üíüëâüî•]\s*(–ó–∞–±—Ä–∞—Ç—å|–ü–æ–ª—É—á–∏—Ç—å|–ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å|–í–∑—è—Ç—å|–ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å|–ó–∞—Ä—è–¥–∏—Ç—å—Å—è)',
    r'^\s*(–ó–∞–±—Ä–∞—Ç—å|–ü–æ–ª—É—á–∏—Ç—å|–ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å|–í–∑—è—Ç—å) [^<\n]{10,80}$',
]

# –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
posts_with_text_links = []

for i, p in enumerate(posts[:420]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ä—ã–µ 420 –ø–æ—Å—Ç–æ–≤
    text = p.get('text', '')
    lines = text.split('\n')
    
    for line in lines:
        for pattern in link_text_patterns:
            if re.search(pattern, line, re.MULTILINE | re.IGNORECASE):
                if i not in [x[0] for x in posts_with_text_links]:
                    posts_with_text_links.append((i, line.strip()[:100]))
                break

output_lines.append("="*80)
output_lines.append(f"–ü–û–°–¢–´ –° –¢–ï–ö–°–¢–û–í–´–ú–ò –ü–†–ò–ó–´–í–ê–ú–ò (–≤–æ–∑–º–æ–∂–Ω—ã–µ –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∏):")
output_lines.append("="*80)
output_lines.append(f"–ù–∞–π–¥–µ–Ω–æ: {len(posts_with_text_links)} –ø–æ—Å—Ç–æ–≤ –∏–∑ 420")
output_lines.append("")

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
output_lines.append("–ü—Ä–∏–º–µ—Ä—ã (–ø–µ—Ä–≤—ã–µ 20):")
for i, (post_idx, line_text) in enumerate(posts_with_text_links[:20], 1):
    output_lines.append(f"{i}. –ü–æ—Å—Ç #{post_idx}: {line_text}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º - –µ—Å—Ç—å –ª–∏ —Ä—è–¥–æ–º URL?
output_lines.append("")
output_lines.append("="*80)
output_lines.append("–ê–ù–ê–õ–ò–ó: –ï—Å—Ç—å –ª–∏ URL —Ä—è–¥–æ–º —Å —ç—Ç–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏?")
output_lines.append("="*80)

count_with_url_nearby = 0
count_without_url = 0

for post_idx, _ in posts_with_text_links[:50]:
    text = posts[post_idx].get('text', '')
    
    # –ò—â–µ–º –ø—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é
    for pattern in link_text_patterns:
        match = re.search(pattern, text, re.MULTILINE | re.IGNORECASE)
        if match:
            # –ò—â–µ–º URL –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ (¬±150 —Å–∏–º–≤–æ–ª–æ–≤)
            pos = match.start()
            context = text[max(0, pos-150):min(len(text), pos+150)]
            
            if 'http' in context or 'cutt.ly' in context:
                count_with_url_nearby += 1
            else:
                count_without_url += 1
            break

output_lines.append(f"–° URL —Ä—è–¥–æ–º: {count_with_url_nearby}")
output_lines.append(f"–ë–ï–ó URL —Ä—è–¥–æ–º: {count_without_url}")

output_lines.append("")
output_lines.append("="*80)
output_lines.append("–í–´–í–û–î:")
output_lines.append("="*80)

if count_without_url > count_with_url_nearby:
    output_lines.append("""
–≠–¢–û –¢–ï–ö–°–¢–û–í–´–ï –°–°–´–õ–ö–ò –ë–ï–ó –†–ê–ó–ú–ï–¢–ö–ò!

–í —Å—Ç–∞—Ä—ã—Ö –ø–æ—Å—Ç–∞—Ö –ø—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é –Ω–∞–ø–∏—Å–∞–Ω—ã –¢–ï–ö–°–¢–û–ú –±–µ–∑ <a href>.
–ù–∞–ø—Ä–∏–º–µ—Ä:
  "‚û°Ô∏è –ó–∞–±—Ä–∞—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–π –ø–∞–∫–µ—Ç"
  
–ù–æ –≤ Telegram —ç—Ç–æ –ù–ï –∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ!

–í –Ω–æ–≤—ã—Ö 80 –ø–æ—Å—Ç–∞—Ö —ç—Ç–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤:
  <a href="URL">–ó–∞–±—Ä–∞—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–π –ø–∞–∫–µ—Ç</a>
  
–ß—Ç–æ –ö–õ–ò–ö–ê–ë–ï–õ–¨–ù–û –≤ Telegram!
""")
else:
    output_lines.append("""
–≠–¢–û –û–ü–ò–°–ê–ù–ò–Ø –†–Ø–î–û–ú –° URL!

–¢–µ–∫—Å—Ç –∏–¥—ë—Ç —Ä—è–¥–æ–º —Å –æ–±—ã—á–Ω—ã–º–∏ URL:
  https://cutt.ly/xxx
  –ó–∞–±—Ä–∞—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–π –ø–∞–∫–µ—Ç
  
–¢–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç —Ç–æ–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ —ç—Ç–æ –ù–ï –≥–∏–ø–µ—Ä—Å—Å—ã–ª–∫–∞.
""")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º
with open('text_links_analysis.txt', 'w', encoding='utf-8') as f:
    f.write('\n'.join(output_lines))

print(f"OK: Found {len(posts_with_text_links)} posts with text links")
print(f"    Saved to text_links_analysis.txt")
