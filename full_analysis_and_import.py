"""
ĞšĞ¾Ğ¼Ğ¿Ğ»ĞµĞºÑĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ‘Ğ” Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ² + Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ 80 Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ².

ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚:
1. Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ² (Ğ½Ğ° Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ÑÑ‚ÑÑ)
2. ĞÑ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ ÑÑÑ‹Ğ»Ğ¾Ğº
3. ĞĞ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²
4. ĞŸĞ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ ÑÑÑ‹Ğ»Ğ¾Ğº
5. Ğ£Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ ÑĞ»Ğ¾Ñ‚Ğ°/ÑÑ‚Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°/Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ñ
"""

import json
import re
from typing import List, Dict, Tuple
from collections import Counter, defaultdict
from datetime import datetime


def extract_links_and_format(post_text: str) -> Dict:
    """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¸Ñ… Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ."""
    result = {
        "links_count": 0,
        "link_formats": [],
        "link_positions": [],
        "bonus_descriptions": []
    }
    
    # Ğ˜Ñ‰ĞµĞ¼ URL
    url_pattern = r'https?://[^\s<>\"\']+|cutt\.ly/[^\s<>\"\']+' 
    urls = re.findall(url_pattern, post_text)
    result["links_count"] = len(urls)
    
    # Ğ˜Ñ‰ĞµĞ¼ Ğ³Ğ¸Ğ¿ĞµÑ€ÑÑÑ‹Ğ»ĞºĞ¸
    hyperlink_pattern = r'<a href=["\']([^"\']+)["\']>([^<]+)</a>'
    hyperlinks = re.findall(hyperlink_pattern, post_text)
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
    if hyperlinks:
        result["link_formats"].append("hyperlink")
    if urls:
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ URL
        for url in urls[:2]:  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 2 ÑÑÑ‹Ğ»ĞºĞ¸
            url_pos = post_text.find(url)
            if url_pos == -1:
                continue
            
            # ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ
            before = post_text[max(0, url_pos-100):url_pos]
            after = post_text[url_pos:min(len(post_text), url_pos+200)]
            
            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ
            rel_pos = url_pos / len(post_text) if len(post_text) > 0 else 0
            if rel_pos < 0.25:
                result["link_positions"].append("Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾")
            elif rel_pos < 0.5:
                result["link_positions"].append("Ğ¿ĞµÑ€Ğ²Ğ°Ñ_Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ°")
            elif rel_pos < 0.75:
                result["link_positions"].append("Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ_Ğ¿Ğ¾Ğ»Ğ¾Ğ²Ğ¸Ğ½Ğ°")
            else:
                result["link_positions"].append("ĞºĞ¾Ğ½ĞµÑ†")
            
            # ĞĞ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ñ
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ 1: URL - Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
            if re.search(r'https?://\S+\s*[-â€”â€“]\s*\w', after):
                result["link_formats"].append("url_dash_desc")
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ 2: URL\nĞ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
            elif re.search(r'https?://\S+\s*\n\s*\w', after):
                result["link_formats"].append("url_newline_desc")
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ 3: ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ URL Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
            elif re.search(r'[\U0001F300-\U0001F9FF]\s*https?://', before[-20:]):
                result["link_formats"].append("emoji_url_desc")
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ 4: Ğ¢ĞµĞºÑÑ‚: URL
            elif re.search(r'\w+:\s*$', before[-30:]):
                result["link_formats"].append("text_colon_url")
            else:
                result["link_formats"].append("plain_url")
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ½ÑƒÑĞ° (Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ ÑÑÑ‹Ğ»ĞºĞ¸)
            bonus_match = re.search(r'https?://\S+\s*[-â€”â€“]?\s*([^\n]{10,100})', after)
            if bonus_match:
                result["bonus_descriptions"].append(bonus_match.group(1).strip()[:80])
    
    return result


def analyze_post_focus(post_text: str) -> Dict:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½Ğ° Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ¿Ğ¾ÑÑ‚."""
    result = {
        "focus": [],
        "slot_mentions": 0,
        "streamer_mentions": 0,
        "multiplier_mentions": 0,
        "bet_mentions": 0,
        "win_mentions": 0,
        "first_focus": None
    }
    
    # Ğ˜Ñ‰ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒ (x123, Ñ…123)
    multiplier_matches = re.findall(r'[xÑ…]\s*\d{2,}', post_text, re.IGNORECASE)
    result["multiplier_mentions"] = len(multiplier_matches)
    
    # Ğ˜Ñ‰ĞµĞ¼ ÑÑƒĞ¼Ğ¼Ñ‹ (123â‚½, 123 Ñ€ÑƒĞ±Ğ»ĞµĞ¹, 123 Ñ€ÑƒĞ±)
    money_matches = re.findall(r'\d[\d\s]*[â‚½Ñ€ÑƒĞ±]', post_text, re.IGNORECASE)
    result["bet_mentions"] = len(money_matches)
    
    # Ğ˜Ñ‰ĞµĞ¼ ÑĞ»Ğ¾Ñ‚Ñ‹ Ğ² code Ñ‚ĞµĞ³Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ¶Ğ¸Ñ€Ğ½Ğ¾Ğ¼
    slot_in_code = re.findall(r'<code>([^<]+)</code>', post_text)
    slot_in_bold = re.findall(r'<b>([A-Za-z][A-Za-z\s]{3,30})</b>', post_text)
    result["slot_mentions"] = len([s for s in slot_in_code + slot_in_bold if re.match(r'^[A-Za-z]', s)])
    
    # Ğ˜Ñ‰ĞµĞ¼ Ğ½Ğ¸ĞºĞ¸ (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ»Ğ°Ñ‚Ğ¸Ğ½ÑĞºĞ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ°)
    streamer_patterns = [
        r'<code>([A-Za-z0-9_]{3,15})</code>',
        r'(?:Ğ½Ğ°Ñˆ|Ğ¸Ğ³Ñ€Ğ¾Ğº|ÑÑ‚Ñ€Ğ¸Ğ¼ĞµÑ€|Ğ¿Ğ°Ñ€ĞµĞ½ÑŒ)\s+([A-Za-z0-9_]{3,15})',
    ]
    for pattern in streamer_patterns:
        matches = re.findall(pattern, post_text, re.IGNORECASE)
        result["streamer_mentions"] += len(matches)
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑ Ğ¿Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ 150 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼
    first_part = post_text[:200].lower()
    
    if re.search(r'[xÑ…]\s*\d{3,}', first_part):
        result["first_focus"] = "multiplier"
    elif re.search(r'\d+\s*[â‚½Ñ€ÑƒĞ±]', first_part):
        result["first_focus"] = "money"
    elif re.search(r'<code>[a-z]', first_part, re.IGNORECASE):
        result["first_focus"] = "streamer_or_slot"
    elif any(word in first_part for word in ["Ğ½ĞµĞ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾", "Ğ²Ğ°Ñƒ", "ÑˆĞ¾Ğº", "wow", "!!!", "???"]):
        result["first_focus"] = "emotion"
    else:
        result["first_focus"] = "story"
    
    return result


def analyze_post_start(post_text: str) -> str:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ñ‚Ğ¸Ğ¿ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¿Ğ¾ÑÑ‚Ğ°."""
    first_line = post_text.split('\n')[0][:100] if post_text else ""
    
    # Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ
    if re.match(r'^[\U0001F300-\U0001F9FF\u2600-\u26FF\u2700-\u27BF]', first_line):
        return "emoji"
    # Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ
    if '?' in first_line[:50]:
        return "question"
    # Ğ§Ğ¸ÑĞ»Ğ°/Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒ
    if re.match(r'^[\d\s]*[xÑ…]?\d', first_line):
        return "numbers"
    # Ğ’Ğ¾ÑĞºĞ»Ğ¸Ñ†Ğ°Ğ½Ğ¸Ğµ
    if '!' in first_line[:30]:
        return "exclamation"
    # Ğ¢ĞµĞ³ HTML Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ
    if first_line.startswith('<'):
        return "html_tag"
    # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚
    return "text"


def parse_telegram_post(msg: dict) -> str:
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Telegram Ğ² HTML Ñ‚ĞµĞºÑÑ‚."""
    text_entities = msg.get('text_entities', [])
    if not text_entities:
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ text
        text = msg.get('text', '')
        if isinstance(text, list):
            return ''.join([e.get('text', '') if isinstance(e, dict) else str(e) for e in text])
        return str(text) if text else ""
    
    full_text = ""
    for entity in text_entities:
        if isinstance(entity, dict):
            text = entity.get('text', '')
            entity_type = entity.get('type', 'plain')
            
            if entity_type == 'bold':
                full_text += f"<b>{text}</b>"
            elif entity_type == 'code':
                full_text += f"<code>{text}</code>"
            elif entity_type == 'italic':
                full_text += f"<i>{text}</i>"
            elif entity_type == 'text_link':
                href = entity.get('href', '')
                full_text += f'<a href="{href}">{text}</a>'
            elif entity_type == 'link':
                full_text += text
            else:
                full_text += text
        elif isinstance(entity, str):
            full_text += entity
    
    return full_text


def main():
    output = []
    
    def log(text=""):
        output.append(text)
    
    log("=" * 80)
    log("ĞšĞĞœĞŸĞ›Ğ•ĞšĞ¡ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ‘ĞĞ—Ğ« Ğ”ĞĞĞĞ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’")
    log("=" * 80)
    log(f"Ğ”Ğ°Ñ‚Ğ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ™ Ğ‘ĞĞ—Ğ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("1. Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ™ Ğ‘ĞĞ—Ğ« (my_posts.json)")
    log("â•" * 80)
    
    try:
        with open('data/my_posts.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
            current_posts = data.get('posts', [])
        log(f"   Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²: {len(current_posts)}")
    except Exception as e:
        log(f"   ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        current_posts = []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’ Ğ˜Ğ— TELEGRAM
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("2. Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’ (result.json)")
    log("â•" * 80)
    
    try:
        with open(r'c:\Users\smike\Downloads\Telegram Desktop\ChatExport_2026-01-21\result.json', 'r', encoding='utf-8') as f:
            telegram_data = json.load(f)
        
        new_posts = []
        for msg in telegram_data.get('messages', []):
            if msg.get('type') != 'message':
                continue
            
            text = parse_telegram_post(msg)
            if len(text) > 100:
                new_posts.append({
                    'text': text,
                    'date': msg.get('date', ''),
                    'id': msg.get('id', 0)
                })
        
        log(f"   Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²: {len(new_posts)}")
    except Exception as e:
        log(f"   ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        new_posts = []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. ĞĞĞĞ›Ğ˜Ğ— Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ™ Ğ‘ĞĞ—Ğ«
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("3. Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ— Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ™ Ğ‘ĞĞ—Ğ«")
    log("â•" * 80)
    
    # Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
    stats = {
        "link_formats": Counter(),
        "link_positions": Counter(),
        "post_starts": Counter(),
        "first_focus": Counter(),
        "bonus_examples": [],
        "link_count_distribution": Counter(),
        "multiplier_in_start": 0,
        "slot_in_start": 0,
        "emotion_start": 0,
    }
    
    for post_data in current_posts:
        text = post_data.get('text', '')
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑÑÑ‹Ğ»Ğ¾Ğº
        links_info = extract_links_and_format(text)
        stats["link_count_distribution"][links_info["links_count"]] += 1
        for fmt in links_info["link_formats"]:
            stats["link_formats"][fmt] += 1
        for pos in links_info["link_positions"]:
            stats["link_positions"][pos] += 1
        for bonus in links_info["bonus_descriptions"][:2]:
            if bonus and len(stats["bonus_examples"]) < 30:
                stats["bonus_examples"].append(bonus)
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°
        start_type = analyze_post_start(text)
        stats["post_starts"][start_type] += 1
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ„Ğ¾ĞºÑƒÑĞ°
        focus = analyze_post_focus(text)
        if focus["first_focus"]:
            stats["first_focus"][focus["first_focus"]] += 1
    
    # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
    log("\nğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ™ Ğ‘ĞĞ—Ğ«:")
    log(f"\n   Ğ’ÑĞµĞ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²: {len(current_posts)}")
    
    log("\n   ğŸ“Œ Ğ¢Ğ˜ĞŸĞ« ĞĞĞ§ĞĞ›Ğ ĞŸĞĞ¡Ğ¢ĞĞ’:")
    for start_type, count in stats["post_starts"].most_common():
        pct = count / len(current_posts) * 100 if current_posts else 0
        log(f"      {start_type}: {count} ({pct:.1f}%)")
    
    log("\n   ğŸ¯ ĞĞ Ğ§Ğ¢Ğ ĞĞŸĞ˜Ğ ĞĞ•Ğ¢Ğ¡Ğ¯ ĞĞĞ§ĞĞ›Ğ ĞŸĞĞ¡Ğ¢Ğ:")
    for focus, count in stats["first_focus"].most_common():
        pct = count / len(current_posts) * 100 if current_posts else 0
        log(f"      {focus}: {count} ({pct:.1f}%)")
    
    log("\n   ğŸ”— Ğ¤ĞĞ ĞœĞĞ¢Ğ« Ğ¡Ğ¡Ğ«Ğ›ĞĞš:")
    for fmt, count in stats["link_formats"].most_common():
        log(f"      {fmt}: {count}")
    
    log("\n   ğŸ“ ĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ˜ Ğ¡Ğ¡Ğ«Ğ›ĞĞš:")
    for pos, count in stats["link_positions"].most_common():
        pct = count / sum(stats["link_positions"].values()) * 100 if stats["link_positions"] else 0
        log(f"      {pos}: {count} ({pct:.1f}%)")
    
    log("\n   ğŸ”¢ ĞšĞĞ›Ğ˜Ğ§Ğ•Ğ¡Ğ¢Ğ’Ğ Ğ¡Ğ¡Ğ«Ğ›ĞĞš Ğ’ ĞŸĞĞ¡Ğ¢Ğ•:")
    for num, count in sorted(stats["link_count_distribution"].items()):
        log(f"      {num} ÑÑÑ‹Ğ»Ğ¾Ğº: {count} Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²")
    
    log("\n   ğŸ’° ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« ĞĞŸĞ˜Ğ¡ĞĞĞ˜Ğ™ Ğ‘ĞĞĞ£Ğ¡ĞĞ’ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 15):")
    for i, bonus in enumerate(stats["bonus_examples"][:15], 1):
        log(f"      {i}. {bonus}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. ĞĞĞĞ›Ğ˜Ğ— ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("4. ĞĞĞĞ›Ğ˜Ğ— ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’ (result.json)")
    log("â•" * 80)
    
    new_stats = {
        "link_formats": Counter(),
        "link_positions": Counter(),
        "post_starts": Counter(),
        "first_focus": Counter(),
        "bonus_examples": [],
    }
    
    for post_data in new_posts:
        text = post_data.get('text', '')
        
        links_info = extract_links_and_format(text)
        for fmt in links_info["link_formats"]:
            new_stats["link_formats"][fmt] += 1
        for pos in links_info["link_positions"]:
            new_stats["link_positions"][pos] += 1
        for bonus in links_info["bonus_descriptions"][:2]:
            if bonus and len(new_stats["bonus_examples"]) < 30:
                new_stats["bonus_examples"].append(bonus)
        
        start_type = analyze_post_start(text)
        new_stats["post_starts"][start_type] += 1
        
        focus = analyze_post_focus(text)
        if focus["first_focus"]:
            new_stats["first_focus"][focus["first_focus"]] += 1
    
    log("\nğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’:")
    log(f"\n   Ğ’ÑĞµĞ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ²: {len(new_posts)}")
    
    log("\n   ğŸ“Œ Ğ¢Ğ˜ĞŸĞ« ĞĞĞ§ĞĞ›Ğ ĞŸĞĞ¡Ğ¢ĞĞ’:")
    for start_type, count in new_stats["post_starts"].most_common():
        pct = count / len(new_posts) * 100 if new_posts else 0
        log(f"      {start_type}: {count} ({pct:.1f}%)")
    
    log("\n   ğŸ¯ ĞĞ Ğ§Ğ¢Ğ ĞĞŸĞ˜Ğ ĞĞ•Ğ¢Ğ¡Ğ¯ ĞĞĞ§ĞĞ›Ğ ĞŸĞĞ¡Ğ¢Ğ:")
    for focus, count in new_stats["first_focus"].most_common():
        pct = count / len(new_posts) * 100 if new_posts else 0
        log(f"      {focus}: {count} ({pct:.1f}%)")
    
    log("\n   ğŸ”— Ğ¤ĞĞ ĞœĞĞ¢Ğ« Ğ¡Ğ¡Ğ«Ğ›ĞĞš:")
    for fmt, count in new_stats["link_formats"].most_common():
        log(f"      {fmt}: {count}")
    
    log("\n   ğŸ“ ĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ˜ Ğ¡Ğ¡Ğ«Ğ›ĞĞš:")
    for pos, count in new_stats["link_positions"].most_common():
        pct = count / sum(new_stats["link_positions"].values()) * 100 if new_stats["link_positions"] else 0
        log(f"      {pos}: {count} ({pct:.1f}%)")
    
    log("\n   ğŸ’° ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« ĞĞŸĞ˜Ğ¡ĞĞĞ˜Ğ™ Ğ‘ĞĞĞ£Ğ¡ĞĞ’ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 15):")
    for i, bonus in enumerate(new_stats["bonus_examples"][:15], 1):
        log(f"      {i}. {bonus}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5. Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ• ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« ĞŸĞĞ¡Ğ¢ĞĞ’
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("5. Ğ”Ğ•Ğ¢ĞĞ›Ğ¬ĞĞ«Ğ• ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« ĞŸĞĞ¡Ğ¢ĞĞ’")
    log("â•" * 80)
    
    log("\nğŸ“ ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ˜Ğ— Ğ¢Ğ•ĞšĞ£Ğ©Ğ•Ğ™ Ğ‘ĞĞ—Ğ« (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5):")
    for i, post_data in enumerate(current_posts[:5], 1):
        text = post_data.get('text', '')
        log(f"\n--- ĞŸĞĞ¡Ğ¢ #{i} ---")
        log(f"ĞĞ°Ñ‡Ğ°Ğ»Ğ¾: {analyze_post_start(text)}")
        log(f"Ğ¤Ğ¾ĞºÑƒÑ: {analyze_post_focus(text)['first_focus']}")
        log(f"Ğ¡ÑÑ‹Ğ»Ğ¾Ğº: {extract_links_and_format(text)['links_count']}")
        log(f"Ğ¢ĞµĞºÑÑ‚ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 300 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²):")
        log(text[:300] + "..." if len(text) > 300 else text)
    
    log("\n\nğŸ“ ĞŸĞ Ğ˜ĞœĞ•Ğ Ğ« Ğ˜Ğ— ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5):")
    for i, post_data in enumerate(new_posts[:5], 1):
        text = post_data.get('text', '')
        log(f"\n--- ĞĞĞ’Ğ«Ğ™ ĞŸĞĞ¡Ğ¢ #{i} ---")
        log(f"ĞĞ°Ñ‡Ğ°Ğ»Ğ¾: {analyze_post_start(text)}")
        log(f"Ğ¤Ğ¾ĞºÑƒÑ: {analyze_post_focus(text)['first_focus']}")
        log(f"Ğ¡ÑÑ‹Ğ»Ğ¾Ğº: {extract_links_and_format(text)['links_count']}")
        log(f"Ğ¢ĞµĞºÑÑ‚ (Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 300 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²):")
        log(text[:300] + "..." if len(text) > 300 else text)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6. Ğ˜ĞœĞŸĞĞ Ğ¢ ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("6. Ğ˜ĞœĞŸĞĞ Ğ¢ 80 ĞĞĞ’Ğ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’ Ğ’ Ğ‘ĞĞ—Ğ£")
    log("â•" * 80)
    
    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ID
    max_id = max([p.get('id', 0) for p in current_posts]) if current_posts else 0
    log(f"\n   Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ID: {max_id}")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹ (Ğ¿Ğ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ 100 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼)
    existing_starts = set()
    for p in current_posts:
        text = p.get('text', '')[:100]
        existing_starts.add(text)
    
    imported = 0
    duplicates = 0
    
    for post_data in new_posts:
        text = post_data.get('text', '')
        text_start = text[:100]
        
        if text_start in existing_starts:
            duplicates += 1
            continue
        
        existing_starts.add(text_start)
        max_id += 1
        
        current_posts.append({
            'text': text,
            'date': post_data.get('date', datetime.now().isoformat()),
            'id': max_id
        })
        imported += 1
    
    log(f"   Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: {imported}")
    log(f"   Ğ”ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ¾: {duplicates}")
    log(f"   Ğ˜Ñ‚Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ² Ğ² Ğ±Ğ°Ğ·Ğµ: {len(current_posts)}")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ
    data['posts'] = current_posts
    
    with open('data/my_posts.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    log(f"\n   âœ… Ğ‘Ğ°Ğ·Ğ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² data/my_posts.json")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7. Ğ’Ğ«Ğ’ĞĞ”Ğ« Ğ˜ Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    log("\n" + "â•" * 80)
    log("7. Ğ’Ğ«Ğ’ĞĞ”Ğ« Ğ˜ Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜")
    log("â•" * 80)
    
    log("""
ğŸ“Š ĞĞĞĞ›Ğ˜Ğ— ĞŸĞĞšĞĞ—ĞĞ›:

1. Ğ¢Ğ˜ĞŸĞ« ĞĞĞ§ĞĞ›Ğ ĞŸĞĞ¡Ğ¢ĞĞ’:
   - Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ² Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ¸Ğ»Ğ¸ Ğ²Ğ¾ÑĞºĞ»Ğ¸Ñ†Ğ°Ğ½Ğ¸Ñ
   - Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ñ€ĞµĞ¶Ğµ, Ğ½Ğ¾ ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹
   - ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ñ Ñ†Ğ¸Ñ„Ñ€/Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¸Ğ½Ñ‚Ñ€Ğ¸Ğ³Ñƒ

2. ĞĞ Ğ§Ğ¢Ğ ĞĞŸĞ˜Ğ ĞĞ®Ğ¢Ğ¡Ğ¯ ĞŸĞĞ¡Ğ¢Ğ«:
   - "emotion" - ÑĞ¼Ğ¾Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ…ÑƒĞº (Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚!)
   - "money" - Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ´ĞµĞ½ÑŒĞ³Ğ°Ñ…/ÑÑ‚Ğ°Ğ²ĞºĞµ
   - "multiplier" - Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»Ğµ
   - "story" - Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ Ñ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸
   - "streamer_or_slot" - Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° ÑÑ‚Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ Ğ¸Ğ»Ğ¸ ÑĞ»Ğ¾Ñ‚Ğµ

3. Ğ¤ĞĞ ĞœĞĞ¢Ğ« Ğ¡Ğ¡Ğ«Ğ›ĞĞš:
   - url_dash_desc: URL - Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ±Ğ¾Ğ½ÑƒÑĞ°
   - url_newline_desc: URL\\nĞ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
   - emoji_url_desc: ğŸ”¥ URL Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
   - hyperlink: <a href="URL">Ñ‚ĞµĞºÑÑ‚</a>

4. ĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ˜ Ğ¡Ğ¡Ğ«Ğ›ĞĞš:
   - Ğ§Ğ°Ñ‰Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ² ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ½Ñ†Ğµ
   - Ğ ĞµĞ¶Ğµ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ (Ğ½Ğ¾ ÑÑ‚Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ğµ!)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ Ğ Ğ•ĞšĞĞœĞ•ĞĞ”ĞĞ¦Ğ˜Ğ˜ Ğ”Ğ›Ğ¯ ĞŸĞ ĞĞœĞŸĞ¢ĞĞ’:

1. Ğ¡Ğ›ĞĞ¢ = Ğ”Ğ•Ğ¢ĞĞ›Ğ¬, ĞĞ• ĞĞ¡ĞĞĞ’Ğ
   âŒ "Ğ¡Ñ‚Ñ€Ğ¾Ğ¹ Ğ¿Ğ¾ÑÑ‚ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ Ñ‚ĞµĞ¼Ñ‹ ÑĞ»Ğ¾Ñ‚Ğ°"
   âœ… "Ğ£Ğ¿Ğ¾Ğ¼ÑĞ½Ğ¸ ÑĞ»Ğ¾Ñ‚ 1 Ñ€Ğ°Ğ· ĞºĞ°Ğº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸"

2. ĞĞĞ§Ğ˜ĞĞĞ¢Ğ¬ Ğ¡ Ğ£ĞĞ˜Ğ’Ğ•Ğ Ğ¡ĞĞ›Ğ¬ĞĞĞ“Ğ Ğ¥Ğ£ĞšĞ:
   âœ… Ğ­Ğ¼Ğ¾Ñ†Ğ¸Ñ: "Ğ­Ñ‚Ğ¾ Ğ½ĞµĞ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾!", "Ğ’Ğ°Ñƒ!"
   âœ… Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: "Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑÑ‚Ğ¾Ğ¸Ñ‚ ÑƒĞ´Ğ°Ñ‡Ğ°?"
   âœ… Ğ¦Ğ¸Ñ„Ñ€Ñ‹: "x5000 â€” Ğ·Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸ ÑÑ‚Ğ¾ Ñ‡Ğ¸ÑĞ»Ğ¾"
   âŒ ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ñ‚Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ´ĞµĞ»Ğ¾Ğ¼

3. Ğ ĞĞ—ĞĞĞĞ‘Ğ ĞĞ—Ğ˜Ğ• ĞŸĞĞ—Ğ˜Ğ¦Ğ˜Ğ™ Ğ¡Ğ¡Ğ«Ğ›ĞĞš:
   - 30% Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ
   - 40% Ğ² ÑĞµÑ€ĞµĞ´Ğ¸Ğ½Ğµ
   - 30% Ğ² ĞºĞ¾Ğ½Ñ†Ğµ

4. Ğ¤ĞĞ ĞœĞĞ¢Ğ« Ğ‘ĞĞĞ£Ğ¡ĞĞ’ Ğ˜Ğ— Ğ Ğ•ĞĞ›Ğ¬ĞĞ«Ğ¥ ĞŸĞĞ¡Ğ¢ĞĞ’:
   - "500 Ğ¿Ğ¾Ğ´Ğ°Ñ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğ¹"
   - "ÑƒĞ´Ğ²Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ 2 500 EUR"
   - "150% Ğ½Ğ° Ğ´ĞµĞ¿ + Ğ´Ğ¾ 30.000 Ñ€ÑƒĞ±Ğ»ĞµĞ¹ ÑĞ²ĞµÑ€Ñ…Ñƒ"
   - "500 Ğ±Ğ¾Ğ½ÑƒÑĞ½Ñ‹Ñ… Ñ€Ğ°ÑƒĞ½Ğ´Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½Ñ‹Ğµ 30 000 ÑĞ²ĞµÑ€Ñ…Ñƒ"
""")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
    with open('full_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(output))
    
    print("OK: Analysis complete! Report saved to full_analysis_report.txt")
    print(f"    Imported {imported} new posts to my_posts.json")


if __name__ == "__main__":
    main()
